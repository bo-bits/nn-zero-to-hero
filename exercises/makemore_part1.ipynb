{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPBam2VF5JO9VDIBubIOW2M",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bo-bits/nn-zero-to-hero/blob/master/exercises/makemore_part1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "E01: train a trigram language model, i.e. take two characters as an input to predict the 3rd one. Feel free to use either counting or a neural net. Evaluate the loss; Did it improve over a bigram model?"
      ],
      "metadata": {
        "id": "s6FNrkKZEiLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Trigram Model\n",
        "# download the names.txt file from github\n",
        "!wget https://raw.githubusercontent.com/karpathy/makemore/master/names.txt\n",
        "\n",
        "words = open('names.txt', 'r').read().splitlines()\n"
      ],
      "metadata": {
        "id": "3hCniXWZE5dA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "993c8911-f8dd-4be3-c917-23939c328efc"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-11-06 13:13:11--  https://raw.githubusercontent.com/karpathy/makemore/master/names.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 228145 (223K) [text/plain]\n",
            "Saving to: ‘names.txt.1’\n",
            "\n",
            "names.txt.1         100%[===================>] 222.80K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2024-11-06 13:13:11 (11.0 MB/s) - ‘names.txt.1’ saved [228145/228145]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dictionary for single and double characters\n",
        "from itertools import islice\n",
        "\n",
        "chars = sorted(list(set(''.join(words))))\n",
        "chars.insert(0, '.')\n",
        "\n",
        "# Initialize stoi for single characters\n",
        "stoi = {char: i for i, char in enumerate(chars)}\n",
        "\n",
        "# Start bigram indices after the single characters\n",
        "bigram_start_idx = len(stoi)\n",
        "\n",
        "# Add bigrams to stoi with unique indices\n",
        "for i, (char1, char2) in enumerate((a + b for a in chars for b in chars), bigram_start_idx):\n",
        "    stoi[char1 + char2] = i\n",
        "\n",
        "# Create itos for reverse mapping\n",
        "itos = {i: s for s, i in stoi.items()}\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "n5wW-UlMvvtL"
      },
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Create the training set of trigrams (x,y)\n",
        "xs, ys = [], []\n",
        "for w in words:\n",
        "  chs = ['.'] + list(w) + ['.']\n",
        "  for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
        "    ix1 = stoi[ch1+ch2]\n",
        "    ix3 = stoi[ch3]\n",
        "    xs.append(ix1)\n",
        "    ys.append(ix3)\n",
        "xs = torch.tensor(xs)\n",
        "ys = torch.tensor(ys)\n",
        "num = xs.nelement()\n",
        "print('number of examples: ', num)"
      ],
      "metadata": {
        "id": "AOF8XEaVjo8E",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a81a5b89-703f-4a58-a733-14deb4a46366"
      },
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of examples:  17129\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize\n",
        "g = torch.Generator().manual_seed(2147483647)\n",
        "W = torch.randn((756, 27), generator=g, requires_grad=True)"
      ],
      "metadata": {
        "id": "sjKj__bdH32m"
      },
      "execution_count": 193,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(200):\n",
        "  # forward pass\n",
        "  logits = W[xs] # predict log-counts\n",
        "  counts = logits.exp() # counts, equivalent to N\n",
        "  probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
        "  loss = -probs[torch.arange(num), ys].log().mean()+ 0.01*(W**2).mean()\n",
        "  # print(f\"Loss: \",loss.item())\n",
        "\n",
        "  # backward pass\n",
        "  W.grad = None # set to zero the gradient\n",
        "  loss.backward()\n",
        "  W.data += -50 * W.grad\n",
        "print(\"Loss: \",loss.item())"
      ],
      "metadata": {
        "id": "op4jJpqQrW6_",
        "outputId": "8041d395-768a-46bd-8dca-d0eae93c16c6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 194,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss:  0.6853259801864624\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bigram model achieves a loss of ~2 while Trigram achieves a loss of ~0.7 for 100 iterations with 0.01 reg and stepsize 50"
      ],
      "metadata": {
        "id": "Yn96NifYJgrI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.arange(xs.nelement())"
      ],
      "metadata": {
        "id": "67Jm2RKyDeGu",
        "outputId": "a97d0823-3a12-42c9-e7eb-6bd78f8a8a29",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 222,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([    0,     1,     2,  ..., 20128, 20129, 20130])"
            ]
          },
          "metadata": {},
          "execution_count": 222
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample from the 'neural net' trigram model\n",
        "g = torch.Generator().manual_seed(2147483647)\n",
        "\n",
        "for i in range(5):\n",
        "  out = []\n",
        "  ix = stoi['..']\n",
        "  char1 = '.'\n",
        "  char2 = '.'\n",
        "  while True:\n",
        "    ix = stoi[char1+char2]\n",
        "    xenc = F.one_hot(torch.tensor([ix]), num_classes=756).float()\n",
        "    logits = xenc @ W # predict log-counts\n",
        "    counts = logits.exp() # counts, equivalent to N\n",
        "    p = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
        "\n",
        "    iy = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
        "    out.append(itos[iy])\n",
        "    char1 = char2\n",
        "    char2 = itos[iy]\n",
        "    if iy == 0:\n",
        "      break\n",
        "  print(''.join(out))"
      ],
      "metadata": {
        "id": "DbeRDg5FoN59",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f482abc-c7cd-48f1-9498-71db83b57002"
      },
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "juwideddpkaqdd.\n",
            "pgzqkygbynrqgjirrwtolcdgoftwzzvsagjpauyfmgadvaaikdbduikrwrmtrdsnjyievylarryzffvmumjhyfottmmj.\n",
            "nfyaszwjhruagq.\n",
            "cohaayaeboffmypjabdihejfmoifbwyfitpvgiasnhsvjihopbuxhddgosfmptpuviczqrjpiufjxhdtgr.\n",
            "nrsla.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "E02: split up the dataset randomly into 80% train set, 10% dev set, 10% test set. Train the bigram and trigram models only on the training set. Evaluate them on dev and test splits. What can you see?"
      ],
      "metadata": {
        "id": "1E_da2qIEtE7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For exercise 2 I created a block_size-independent implementation of dataset\n",
        "# to be able to call it for both bigram and trigram model\n",
        "\n",
        "def dataset(block_size):\n",
        "  xs, ys = [], []\n",
        "  for w in words:\n",
        "    context = [0] * block_size\n",
        "    for ch in w + '.':\n",
        "      ix = stoi[ch]\n",
        "      s=''\n",
        "      for i in context:\n",
        "        s += itos[i]\n",
        "      xs.append(stoi[s])\n",
        "      ys.append(ix)\n",
        "      context = context[1:] + [ix] # crop and append\n",
        "  xs = torch.tensor(xs)\n",
        "  ys = torch.tensor(ys)\n",
        "  return xs, ys"
      ],
      "metadata": {
        "id": "bkvcw6jXVlZD"
      },
      "execution_count": 223,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, random_split\n",
        "import torch.nn.functional as F\n",
        "from itertools import islice\n",
        "from numpy import linspace\n",
        "\n",
        "\n",
        "# Function to divide dataset into 0.8 train 0.1 dev 0.1 test\n",
        "def split_dataset(xs, ys):\n",
        "  dataset = TensorDataset(xs, ys)\n",
        "\n",
        "  # Define the split sizes\n",
        "  train_size = int(0.8 * len(dataset))\n",
        "  dev_size = int(0.1 * len(dataset))\n",
        "  test_size = len(dataset) - train_size - dev_size\n",
        "\n",
        "  # Split the dataset\n",
        "  train_dataset, dev_dataset, test_dataset = random_split(dataset, [train_size, dev_size, test_size])\n",
        "\n",
        "  # Access indices from the subsets\n",
        "  train_indices = train_dataset.indices\n",
        "  dev_indices = dev_dataset.indices\n",
        "  test_indices = test_dataset.indices\n",
        "\n",
        "  # Extract training tensors\n",
        "  X_train = xs[train_indices]\n",
        "  y_train = ys[train_indices]\n",
        "\n",
        "  # Extract testing tensors\n",
        "  X_dev = xs[dev_indices]\n",
        "  y_dev = ys[dev_indices]\n",
        "\n",
        "  # Extract testing tensors\n",
        "  X_test = xs[test_indices]\n",
        "  y_test = ys[test_indices]\n",
        "\n",
        "  return X_train, y_train, X_dev, y_dev, X_test, y_test\n"
      ],
      "metadata": {
        "id": "lcbUvWfmCvfy"
      },
      "execution_count": 224,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to train\n",
        "def train(iter, reg):\n",
        "  for i in range(iter):\n",
        "    # forward pass\n",
        "    logits = W[X_train] # predict log-counts\n",
        "    counts = logits.exp() # counts, equivalent to N\n",
        "    probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
        "    loss = -probs[torch.arange(X_train.nelement()), y_train].log().mean()+ reg*(W**2).mean()\n",
        "    # print(f\"Loss: \",loss.item())\n",
        "\n",
        "    # backward pass\n",
        "    W.grad = None # set to zero the gradient\n",
        "    loss.backward()\n",
        "    W.data += -50 * W.grad\n",
        "  return loss"
      ],
      "metadata": {
        "id": "o27nVQV8rAQ-"
      },
      "execution_count": 225,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to evaluate performance of bigram\n",
        "def eval_bigram(X, y, reg):\n",
        "  logits = W[X] # predict log-counts\n",
        "  counts = logits.exp() # counts, equivalent to N\n",
        "  probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
        "  loss = -probs[torch.arange(X.nelement()), y].log().mean()+ reg*(W**2).mean()\n",
        "  return loss"
      ],
      "metadata": {
        "id": "jIWFDLiuqdBk"
      },
      "execution_count": 226,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bigram Implementation\n",
        "\n",
        "# Create Datasets\n",
        "xs, ys = dataset(1)\n",
        "X_train, y_train, X_dev, y_dev, X_test, y_test = split_dataset(xs, ys)\n",
        "\n",
        "regs = [10, 1, 0.1, 0.01, 0.001, 0.0001, 1e-5, 1e-6, 1e-7]\n",
        "tloss, eloss = [], []\n",
        "\n",
        "for reg in regs:\n",
        "\n",
        "  # Initialize\n",
        "  g = torch.Generator().manual_seed(2147483647)\n",
        "  W = torch.randn((27, 27), generator=g, requires_grad=True)\n",
        "\n",
        "  # Train\n",
        "  loss1 = train(200, reg)\n",
        "  print(\"\\nReg: \",reg, \"\\n Train Loss: \",loss1.item())\n",
        "  tloss.append(loss1.item())\n",
        "\n",
        "  # Evaluate\n",
        "  loss2 = eval_bigram(X_dev, y_dev, reg)\n",
        "  print(\" Eval Loss: \", loss2.item())\n",
        "  eloss.append(loss2.item())\n",
        "\n",
        "# Test\n",
        "test_loss = eval_bigram(X_test, y_test, reg)\n",
        "print(\"\\n Test Loss: \", test_loss.item())"
      ],
      "metadata": {
        "id": "gNKt-CT-Y4_L",
        "outputId": "b948cbe1-fb67-410a-e92b-e96a0a7857d5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 227,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Reg:  10 \n",
            " Train Loss:  3.1518807411193848\n",
            " Eval Loss:  3.1599464416503906\n",
            "\n",
            "Reg:  1 \n",
            " Train Loss:  2.6264991760253906\n",
            " Eval Loss:  2.649312973022461\n",
            "\n",
            "Reg:  0.1 \n",
            " Train Loss:  2.118109941482544\n",
            " Eval Loss:  2.1434051990509033\n",
            "\n",
            "Reg:  0.01 \n",
            " Train Loss:  1.948878288269043\n",
            " Eval Loss:  1.9749367237091064\n",
            "\n",
            "Reg:  0.001 \n",
            " Train Loss:  1.9162352085113525\n",
            " Eval Loss:  1.942584753036499\n",
            "\n",
            "Reg:  0.0001 \n",
            " Train Loss:  1.9123629331588745\n",
            " Eval Loss:  1.938744068145752\n",
            "\n",
            "Reg:  1e-05 \n",
            " Train Loss:  1.9119677543640137\n",
            " Eval Loss:  1.9383524656295776\n",
            "\n",
            "Reg:  1e-06 \n",
            " Train Loss:  1.9119285345077515\n",
            " Eval Loss:  1.9383131265640259\n",
            "\n",
            "Reg:  1e-07 \n",
            " Train Loss:  1.9119243621826172\n",
            " Eval Loss:  1.9383093118667603\n",
            "\n",
            " Test Loss:  1.920753002166748\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to evaluate performance of triigram\n",
        "def eval_trigram(X, y, reg):\n",
        "  logits = W[X] # predict log-counts\n",
        "  counts = logits.exp() # counts, equivalent to N\n",
        "  probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
        "  loss = -probs[torch.arange(X.nelement()), y].log().mean()+ reg*(W**2).mean()\n",
        "  return loss"
      ],
      "metadata": {
        "id": "DipseY3d4a_U"
      },
      "execution_count": 213,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Trigram Implementation\n",
        "\n",
        "# Create Datasets\n",
        "xs, ys = dataset(2)\n",
        "X_train, y_train, X_dev, y_dev, X_test, y_test = split_dataset(xs, ys)\n",
        "\n",
        "regs = [10, 1, 0.1, 0.01, 0.001, 0.0001, 0.00001, 10e-7]\n",
        "tloss, eloss = [], []\n",
        "\n",
        "for reg in regs:\n",
        "\n",
        "  # Initialize\n",
        "  g = torch.Generator().manual_seed(2147483647)\n",
        "  W = torch.randn((756, 27), generator=g, requires_grad=True)\n",
        "\n",
        "  # Train\n",
        "  loss = train(200, reg)\n",
        "  print(\"\\nReg: \",reg, \"\\n Train Loss: \",loss.item())\n",
        "  tloss.append(loss.item())\n",
        "\n",
        "  # Evaluate\n",
        "  loss = eval_trigram(X_dev, y_dev, reg)\n",
        "  print(\" Eval Loss: \", loss.item())\n",
        "  eloss.append(loss.item())\n",
        "\n",
        "  # Test\n",
        "  loss = eval(X_test, y_test, reg)\n",
        "  print(\"\\n Test Loss: \", loss.item())"
      ],
      "metadata": {
        "id": "DizE1s7n1W0o",
        "outputId": "2726bd43-2158-4cc2-caf4-c17794de0c03",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 228,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Reg:  10 \n",
            " Train Loss:  2.083393096923828\n",
            " Eval Loss:  2.098149299621582\n",
            "\n",
            " Test Loss:  2.097676992416382\n",
            "\n",
            "Reg:  1 \n",
            " Train Loss:  1.3488225936889648\n",
            " Eval Loss:  1.3733298778533936\n",
            "\n",
            " Test Loss:  1.3521088361740112\n",
            "\n",
            "Reg:  0.1 \n",
            " Train Loss:  1.0763643980026245\n",
            " Eval Loss:  1.1066514253616333\n",
            "\n",
            " Test Loss:  1.0784415006637573\n",
            "\n",
            "Reg:  0.01 \n",
            " Train Loss:  0.9770079255104065\n",
            " Eval Loss:  1.0080140829086304\n",
            "\n",
            " Test Loss:  0.9787640571594238\n",
            "\n",
            "Reg:  0.001 \n",
            " Train Loss:  0.9654039740562439\n",
            " Eval Loss:  0.9964771270751953\n",
            "\n",
            " Test Loss:  0.9671182632446289\n",
            "\n",
            "Reg:  0.0001 \n",
            " Train Loss:  0.964225172996521\n",
            " Eval Loss:  0.9953050017356873\n",
            "\n",
            " Test Loss:  0.9659352898597717\n",
            "\n",
            "Reg:  1e-05 \n",
            " Train Loss:  0.9641072154045105\n",
            " Eval Loss:  0.9951876997947693\n",
            "\n",
            " Test Loss:  0.9658167362213135\n",
            "\n",
            "Reg:  1e-06 \n",
            " Train Loss:  0.9640952944755554\n",
            " Eval Loss:  0.995175838470459\n",
            "\n",
            " Test Loss:  0.965804934501648\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Performance on test and dev sets is slightly worse than on the training set"
      ],
      "metadata": {
        "id": "UgxwIpPEO_L8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "E03: use the dev set to tune the strength of smoothing (or regularization) for the trigram model - i.e. try many possibilities and see which one works best based on the dev set loss. What patterns can you see in the train and dev set loss as you tune this strength? Take the best setting of the smoothing and evaluate on the test set once and at the end. How good of a loss do you achieve?"
      ],
      "metadata": {
        "id": "Nm5lYMRQEwyJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "E04: we saw that our 1-hot vectors merely select a row of W, so producing these vectors explicitly feels wasteful. Can you delete our use of F.one_hot in favor of simply indexing into rows of W?"
      ],
      "metadata": {
        "id": "OOzpQ4y7Ez99"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize and train model\n",
        "def train_4(reg=0.001):\n",
        "  # Initialize Network\n",
        "  g = torch.Generator().manual_seed(2147483647)\n",
        "  W = torch.randn((756, 27), generator=g, requires_grad=True)\n",
        "\n",
        "  for k in range(100):\n",
        "    # forward pass\n",
        "    logits = W[X_train] # predict log-counts\n",
        "    counts = logits.exp() # counts, equivalent to N\n",
        "    probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
        "    loss = -probs[torch.arange(num_train), y_train].log().mean()\n",
        "    # print(f\"Loss: \",loss.item())\n",
        "\n",
        "    # backward pass\n",
        "    W.grad = None # set to zero the gradient\n",
        "    loss.backward()\n",
        "    W.data += -50 * W.grad\n",
        "  return loss.item()\n",
        "\n",
        "loss = train_4()\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g9L5aRuOmN0n",
        "outputId": "38f50a02-0de1-484a-e241-91cc79cae03d"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6927981972694397\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "E05: look up and use F.cross_entropy instead. You should achieve the same result. Can you think of why we'd prefer to use F.cross_entropy instead?"
      ],
      "metadata": {
        "id": "8VFHbjP8E1Zn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculates the cross entropy between two distributions (predicted and label)\n",
        "It does so by calculating -sum([p[i]*log2(q[i]) for i in range(len(p))])\n",
        "compared to our previous implementation, which took exponent, normalized and then took log and mean of output, cross entropy only takes the product of p and log(q) and integrates softmax and log\n",
        "\n",
        "Why do we prefer cross entropy?\n",
        "More efficient. less computations."
      ],
      "metadata": {
        "id": "D-l1HID6vwS8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize and train model\n",
        "def train_5(reg=0.001):\n",
        "  # Initialize Network\n",
        "  g = torch.Generator().manual_seed(2147483647)\n",
        "  W = torch.randn((756, 27), generator=g, requires_grad=True)\n",
        "\n",
        "  for k in range(100):\n",
        "    # forward pass\n",
        "    logits = W[X_train] # predict log-counts\n",
        "    loss = F.cross_entropy(logits, y_train)\n",
        "    # print(f\"Loss: \",loss.item())\n",
        "\n",
        "    # backward pass\n",
        "    W.grad = None # set to zero the gradient\n",
        "    loss.backward()\n",
        "    W.data += -50 * W.grad\n",
        "  return loss.item()\n",
        "\n",
        "loss = train_4()\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wRAtwAxktBIf",
        "outputId": "7fd17d58-1ce6-4c62-c46d-f07d9cadf5a7"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6927981972694397\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "E06: meta-exercise! Think of a fun/interesting exercise and complete it."
      ],
      "metadata": {
        "id": "E3yrXzL7E2oR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bd75241zCTcw"
      },
      "outputs": [],
      "source": []
    }
  ]
}