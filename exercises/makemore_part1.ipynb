{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOdhEXjakF3ckKkUujgdaEY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bo-bits/nn-zero-to-hero/blob/master/exercises/makemore_part1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "E01: train a trigram language model, i.e. take two characters as an input to predict the 3rd one. Feel free to use either counting or a neural net. Evaluate the loss; Did it improve over a bigram model?"
      ],
      "metadata": {
        "id": "s6FNrkKZEiLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Trigram Model\n",
        "# W = [(26*26)+1 x (26*26)+1] = [677, 677]\n",
        "\n",
        "words = open('names.txt', 'r').read().splitlines()\n",
        "\n"
      ],
      "metadata": {
        "id": "3hCniXWZE5dA"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Creates a sorted list of all characters in words\n",
        "chars = sorted(list(set(''.join(words))))\n",
        "\n",
        "# builds a dictionary where the key (s) is each character from chars,\n",
        "# and the value (i+1) is the index of that character plus 1.\n",
        "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
        "stoi['.'] = 0\n",
        "# reverse dictionary to go from index to s\n",
        "itos = {i:s for s,i in stoi.items()}\n",
        "\n",
        "# Create a dictionary where each key (b) is a unique bigram\n",
        "# The dictionary must include bigrams with '.'\n",
        "null = chars.insert(0,'.')\n",
        "# btoi = {char1 + char2: i for i, (char1, char2) in enumerate(((a, b) for a in chars for b in chars), 0)}\n",
        "\n",
        "# # My implementation of the same dictionary (which works, but not the best)\n",
        "btoi = {}\n",
        "i = 0\n",
        "for char1 in chars:\n",
        "  btoi[char1] = i+1\n",
        "  i += 1\n",
        "  for char2 in chars:\n",
        "    bgrm = char1 + char2\n",
        "    btoi[bgrm] = i+1\n",
        "    i += 1\n",
        "itob = {i:b for b,i in btoi.items()}"
      ],
      "metadata": {
        "id": "kvMxidUHG5aY"
      },
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create the training set of trigrams (x,y)\n",
        "xs, ys = [], []\n",
        "for w in words[:1]:\n",
        "  chs = ['.'] + list(w) + ['.']\n",
        "  for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
        "    ix1 = btoi[ch1+ch2]\n",
        "    ix3 = stoi[ch3]\n",
        "    xs.append(ix1)\n",
        "    ys.append(ix3)\n",
        "xs = torch.tensor(xs)\n",
        "ys = torch.tensor(ys)\n",
        "print(btoi)\n",
        "\n",
        "\n",
        "\n",
        "# initialize the 'network'\n",
        "g = torch.Generator().manual_seed(2147483647)\n",
        "W = torch.randn((729, 27), generator=g, requires_grad=True)"
      ],
      "metadata": {
        "id": "ti42FTAubHwu",
        "outputId": "74134085-86a3-4935-ec10-591ea5802499",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'.': 1, '..': 2, '.a': 3, '.b': 4, '.c': 5, '.d': 6, '.e': 7, '.f': 8, '.g': 9, '.h': 10, '.i': 11, '.j': 12, '.k': 13, '.l': 14, '.m': 15, '.n': 16, '.o': 17, '.p': 18, '.q': 19, '.r': 20, '.s': 21, '.t': 22, '.u': 23, '.v': 24, '.w': 25, '.x': 26, '.y': 27, '.z': 28, 'a': 29, 'a.': 30, 'aa': 31, 'ab': 32, 'ac': 33, 'ad': 34, 'ae': 35, 'af': 36, 'ag': 37, 'ah': 38, 'ai': 39, 'aj': 40, 'ak': 41, 'al': 42, 'am': 43, 'an': 44, 'ao': 45, 'ap': 46, 'aq': 47, 'ar': 48, 'as': 49, 'at': 50, 'au': 51, 'av': 52, 'aw': 53, 'ax': 54, 'ay': 55, 'az': 56, 'b': 57, 'b.': 58, 'ba': 59, 'bb': 60, 'bc': 61, 'bd': 62, 'be': 63, 'bf': 64, 'bg': 65, 'bh': 66, 'bi': 67, 'bj': 68, 'bk': 69, 'bl': 70, 'bm': 71, 'bn': 72, 'bo': 73, 'bp': 74, 'bq': 75, 'br': 76, 'bs': 77, 'bt': 78, 'bu': 79, 'bv': 80, 'bw': 81, 'bx': 82, 'by': 83, 'bz': 84, 'c': 85, 'c.': 86, 'ca': 87, 'cb': 88, 'cc': 89, 'cd': 90, 'ce': 91, 'cf': 92, 'cg': 93, 'ch': 94, 'ci': 95, 'cj': 96, 'ck': 97, 'cl': 98, 'cm': 99, 'cn': 100, 'co': 101, 'cp': 102, 'cq': 103, 'cr': 104, 'cs': 105, 'ct': 106, 'cu': 107, 'cv': 108, 'cw': 109, 'cx': 110, 'cy': 111, 'cz': 112, 'd': 113, 'd.': 114, 'da': 115, 'db': 116, 'dc': 117, 'dd': 118, 'de': 119, 'df': 120, 'dg': 121, 'dh': 122, 'di': 123, 'dj': 124, 'dk': 125, 'dl': 126, 'dm': 127, 'dn': 128, 'do': 129, 'dp': 130, 'dq': 131, 'dr': 132, 'ds': 133, 'dt': 134, 'du': 135, 'dv': 136, 'dw': 137, 'dx': 138, 'dy': 139, 'dz': 140, 'e': 141, 'e.': 142, 'ea': 143, 'eb': 144, 'ec': 145, 'ed': 146, 'ee': 147, 'ef': 148, 'eg': 149, 'eh': 150, 'ei': 151, 'ej': 152, 'ek': 153, 'el': 154, 'em': 155, 'en': 156, 'eo': 157, 'ep': 158, 'eq': 159, 'er': 160, 'es': 161, 'et': 162, 'eu': 163, 'ev': 164, 'ew': 165, 'ex': 166, 'ey': 167, 'ez': 168, 'f': 169, 'f.': 170, 'fa': 171, 'fb': 172, 'fc': 173, 'fd': 174, 'fe': 175, 'ff': 176, 'fg': 177, 'fh': 178, 'fi': 179, 'fj': 180, 'fk': 181, 'fl': 182, 'fm': 183, 'fn': 184, 'fo': 185, 'fp': 186, 'fq': 187, 'fr': 188, 'fs': 189, 'ft': 190, 'fu': 191, 'fv': 192, 'fw': 193, 'fx': 194, 'fy': 195, 'fz': 196, 'g': 197, 'g.': 198, 'ga': 199, 'gb': 200, 'gc': 201, 'gd': 202, 'ge': 203, 'gf': 204, 'gg': 205, 'gh': 206, 'gi': 207, 'gj': 208, 'gk': 209, 'gl': 210, 'gm': 211, 'gn': 212, 'go': 213, 'gp': 214, 'gq': 215, 'gr': 216, 'gs': 217, 'gt': 218, 'gu': 219, 'gv': 220, 'gw': 221, 'gx': 222, 'gy': 223, 'gz': 224, 'h': 225, 'h.': 226, 'ha': 227, 'hb': 228, 'hc': 229, 'hd': 230, 'he': 231, 'hf': 232, 'hg': 233, 'hh': 234, 'hi': 235, 'hj': 236, 'hk': 237, 'hl': 238, 'hm': 239, 'hn': 240, 'ho': 241, 'hp': 242, 'hq': 243, 'hr': 244, 'hs': 245, 'ht': 246, 'hu': 247, 'hv': 248, 'hw': 249, 'hx': 250, 'hy': 251, 'hz': 252, 'i': 253, 'i.': 254, 'ia': 255, 'ib': 256, 'ic': 257, 'id': 258, 'ie': 259, 'if': 260, 'ig': 261, 'ih': 262, 'ii': 263, 'ij': 264, 'ik': 265, 'il': 266, 'im': 267, 'in': 268, 'io': 269, 'ip': 270, 'iq': 271, 'ir': 272, 'is': 273, 'it': 274, 'iu': 275, 'iv': 276, 'iw': 277, 'ix': 278, 'iy': 279, 'iz': 280, 'j': 281, 'j.': 282, 'ja': 283, 'jb': 284, 'jc': 285, 'jd': 286, 'je': 287, 'jf': 288, 'jg': 289, 'jh': 290, 'ji': 291, 'jj': 292, 'jk': 293, 'jl': 294, 'jm': 295, 'jn': 296, 'jo': 297, 'jp': 298, 'jq': 299, 'jr': 300, 'js': 301, 'jt': 302, 'ju': 303, 'jv': 304, 'jw': 305, 'jx': 306, 'jy': 307, 'jz': 308, 'k': 309, 'k.': 310, 'ka': 311, 'kb': 312, 'kc': 313, 'kd': 314, 'ke': 315, 'kf': 316, 'kg': 317, 'kh': 318, 'ki': 319, 'kj': 320, 'kk': 321, 'kl': 322, 'km': 323, 'kn': 324, 'ko': 325, 'kp': 326, 'kq': 327, 'kr': 328, 'ks': 329, 'kt': 330, 'ku': 331, 'kv': 332, 'kw': 333, 'kx': 334, 'ky': 335, 'kz': 336, 'l': 337, 'l.': 338, 'la': 339, 'lb': 340, 'lc': 341, 'ld': 342, 'le': 343, 'lf': 344, 'lg': 345, 'lh': 346, 'li': 347, 'lj': 348, 'lk': 349, 'll': 350, 'lm': 351, 'ln': 352, 'lo': 353, 'lp': 354, 'lq': 355, 'lr': 356, 'ls': 357, 'lt': 358, 'lu': 359, 'lv': 360, 'lw': 361, 'lx': 362, 'ly': 363, 'lz': 364, 'm': 365, 'm.': 366, 'ma': 367, 'mb': 368, 'mc': 369, 'md': 370, 'me': 371, 'mf': 372, 'mg': 373, 'mh': 374, 'mi': 375, 'mj': 376, 'mk': 377, 'ml': 378, 'mm': 379, 'mn': 380, 'mo': 381, 'mp': 382, 'mq': 383, 'mr': 384, 'ms': 385, 'mt': 386, 'mu': 387, 'mv': 388, 'mw': 389, 'mx': 390, 'my': 391, 'mz': 392, 'n': 393, 'n.': 394, 'na': 395, 'nb': 396, 'nc': 397, 'nd': 398, 'ne': 399, 'nf': 400, 'ng': 401, 'nh': 402, 'ni': 403, 'nj': 404, 'nk': 405, 'nl': 406, 'nm': 407, 'nn': 408, 'no': 409, 'np': 410, 'nq': 411, 'nr': 412, 'ns': 413, 'nt': 414, 'nu': 415, 'nv': 416, 'nw': 417, 'nx': 418, 'ny': 419, 'nz': 420, 'o': 421, 'o.': 422, 'oa': 423, 'ob': 424, 'oc': 425, 'od': 426, 'oe': 427, 'of': 428, 'og': 429, 'oh': 430, 'oi': 431, 'oj': 432, 'ok': 433, 'ol': 434, 'om': 435, 'on': 436, 'oo': 437, 'op': 438, 'oq': 439, 'or': 440, 'os': 441, 'ot': 442, 'ou': 443, 'ov': 444, 'ow': 445, 'ox': 446, 'oy': 447, 'oz': 448, 'p': 449, 'p.': 450, 'pa': 451, 'pb': 452, 'pc': 453, 'pd': 454, 'pe': 455, 'pf': 456, 'pg': 457, 'ph': 458, 'pi': 459, 'pj': 460, 'pk': 461, 'pl': 462, 'pm': 463, 'pn': 464, 'po': 465, 'pp': 466, 'pq': 467, 'pr': 468, 'ps': 469, 'pt': 470, 'pu': 471, 'pv': 472, 'pw': 473, 'px': 474, 'py': 475, 'pz': 476, 'q': 477, 'q.': 478, 'qa': 479, 'qb': 480, 'qc': 481, 'qd': 482, 'qe': 483, 'qf': 484, 'qg': 485, 'qh': 486, 'qi': 487, 'qj': 488, 'qk': 489, 'ql': 490, 'qm': 491, 'qn': 492, 'qo': 493, 'qp': 494, 'qq': 495, 'qr': 496, 'qs': 497, 'qt': 498, 'qu': 499, 'qv': 500, 'qw': 501, 'qx': 502, 'qy': 503, 'qz': 504, 'r': 505, 'r.': 506, 'ra': 507, 'rb': 508, 'rc': 509, 'rd': 510, 're': 511, 'rf': 512, 'rg': 513, 'rh': 514, 'ri': 515, 'rj': 516, 'rk': 517, 'rl': 518, 'rm': 519, 'rn': 520, 'ro': 521, 'rp': 522, 'rq': 523, 'rr': 524, 'rs': 525, 'rt': 526, 'ru': 527, 'rv': 528, 'rw': 529, 'rx': 530, 'ry': 531, 'rz': 532, 's': 533, 's.': 534, 'sa': 535, 'sb': 536, 'sc': 537, 'sd': 538, 'se': 539, 'sf': 540, 'sg': 541, 'sh': 542, 'si': 543, 'sj': 544, 'sk': 545, 'sl': 546, 'sm': 547, 'sn': 548, 'so': 549, 'sp': 550, 'sq': 551, 'sr': 552, 'ss': 553, 'st': 554, 'su': 555, 'sv': 556, 'sw': 557, 'sx': 558, 'sy': 559, 'sz': 560, 't': 561, 't.': 562, 'ta': 563, 'tb': 564, 'tc': 565, 'td': 566, 'te': 567, 'tf': 568, 'tg': 569, 'th': 570, 'ti': 571, 'tj': 572, 'tk': 573, 'tl': 574, 'tm': 575, 'tn': 576, 'to': 577, 'tp': 578, 'tq': 579, 'tr': 580, 'ts': 581, 'tt': 582, 'tu': 583, 'tv': 584, 'tw': 585, 'tx': 586, 'ty': 587, 'tz': 588, 'u': 589, 'u.': 590, 'ua': 591, 'ub': 592, 'uc': 593, 'ud': 594, 'ue': 595, 'uf': 596, 'ug': 597, 'uh': 598, 'ui': 599, 'uj': 600, 'uk': 601, 'ul': 602, 'um': 603, 'un': 604, 'uo': 605, 'up': 606, 'uq': 607, 'ur': 608, 'us': 609, 'ut': 610, 'uu': 611, 'uv': 612, 'uw': 613, 'ux': 614, 'uy': 615, 'uz': 616, 'v': 617, 'v.': 618, 'va': 619, 'vb': 620, 'vc': 621, 'vd': 622, 've': 623, 'vf': 624, 'vg': 625, 'vh': 626, 'vi': 627, 'vj': 628, 'vk': 629, 'vl': 630, 'vm': 631, 'vn': 632, 'vo': 633, 'vp': 634, 'vq': 635, 'vr': 636, 'vs': 637, 'vt': 638, 'vu': 639, 'vv': 640, 'vw': 641, 'vx': 642, 'vy': 643, 'vz': 644, 'w': 645, 'w.': 646, 'wa': 647, 'wb': 648, 'wc': 649, 'wd': 650, 'we': 651, 'wf': 652, 'wg': 653, 'wh': 654, 'wi': 655, 'wj': 656, 'wk': 657, 'wl': 658, 'wm': 659, 'wn': 660, 'wo': 661, 'wp': 662, 'wq': 663, 'wr': 664, 'ws': 665, 'wt': 666, 'wu': 667, 'wv': 668, 'ww': 669, 'wx': 670, 'wy': 671, 'wz': 672, 'x': 673, 'x.': 674, 'xa': 675, 'xb': 676, 'xc': 677, 'xd': 678, 'xe': 679, 'xf': 680, 'xg': 681, 'xh': 682, 'xi': 683, 'xj': 684, 'xk': 685, 'xl': 686, 'xm': 687, 'xn': 688, 'xo': 689, 'xp': 690, 'xq': 691, 'xr': 692, 'xs': 693, 'xt': 694, 'xu': 695, 'xv': 696, 'xw': 697, 'xx': 698, 'xy': 699, 'xz': 700, 'y': 701, 'y.': 702, 'ya': 703, 'yb': 704, 'yc': 705, 'yd': 706, 'ye': 707, 'yf': 708, 'yg': 709, 'yh': 710, 'yi': 711, 'yj': 712, 'yk': 713, 'yl': 714, 'ym': 715, 'yn': 716, 'yo': 717, 'yp': 718, 'yq': 719, 'yr': 720, 'ys': 721, 'yt': 722, 'yu': 723, 'yv': 724, 'yw': 725, 'yx': 726, 'yy': 727, 'yz': 728, 'z': 729, 'z.': 730, 'za': 731, 'zb': 732, 'zc': 733, 'zd': 734, 'ze': 735, 'zf': 736, 'zg': 737, 'zh': 738, 'zi': 739, 'zj': 740, 'zk': 741, 'zl': 742, 'zm': 743, 'zn': 744, 'zo': 745, 'zp': 746, 'zq': 747, 'zr': 748, 'zs': 749, 'zt': 750, 'zu': 751, 'zv': 752, 'zw': 753, 'zx': 754, 'zy': 755, 'zz': 756}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "xenc = F.one_hot(xs, num_classes=729).float()\n",
        "\n",
        "# Softmax\n",
        "logits = xenc @ W # log-counts\n",
        "counts = logits.exp() # equivalent N\n",
        "counts\n",
        "# Normalize\n",
        "probs = counts / counts.sum(1, keepdims=True)\n",
        "\n",
        "probs[0].sum()"
      ],
      "metadata": {
        "id": "Kn25hzaJdDR-",
        "outputId": "8c1be7c0-849f-4617-8564-d2550c752a7d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.0000, grad_fn=<SumBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 156
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Equivalent to Softmax\n",
        "logits = xenc @ W # log-counts\n",
        "counts = logits.exp() # equivalent N\n",
        "counts\n",
        "# Normalize\n",
        "probs = counts / counts.sum(1, keepdims=True)\n"
      ],
      "metadata": {
        "id": "nhv2JWQmiaYv"
      },
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculates the average negative log likelihood for a single example\n",
        "# what probability the model assigned to the correct output, take the log (high prob -> 0)\n",
        "# take negative, to make output positive\n",
        "# negative log likelihood is an indicator of how well the NN performed, we want to minimize\n",
        "nlls = torch.zeros(4)\n",
        "for i in range(4):\n",
        "  # i-th bigram:\n",
        "  x = xs[i].item() # input character index\n",
        "  y = ys[i].item() # label character index\n",
        "  print('--------')\n",
        "  print(f'bigram example {i+1}: {itob[x]}{itos[y]} (indexes {x},{y})')\n",
        "  print('input to the neural net:', x)\n",
        "  print('output probabilities from the neural net:', probs[i])\n",
        "  print('label (actual next character):', y)\n",
        "  p = probs[i, y]\n",
        "  print('probability assigned by the net to the the correct character:', p.item())\n",
        "  logp = torch.log(p)\n",
        "  print('log likelihood:', logp.item())\n",
        "  nll = -logp\n",
        "  print('negative log likelihood:', nll.item())\n",
        "  nlls[i] = nll\n",
        "\n",
        "print('=========')\n",
        "print('average negative log likelihood, i.e. loss =', nlls.mean().item())"
      ],
      "metadata": {
        "collapsed": true,
        "id": "XMoSLxVSi_CF",
        "outputId": "3f6cb1bd-6814-427c-cee2-56102ee037ae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------\n",
            "bigram example 1: .ma (indexes 15,1)\n",
            "input to the neural net: 15\n",
            "output probabilities from the neural net: tensor([0.0111, 0.0882, 0.0054, 0.0511, 0.0396, 0.0089, 0.0636, 0.1978, 0.0163,\n",
            "        0.0113, 0.0030, 0.0832, 0.0526, 0.0309, 0.0196, 0.0121, 0.0299, 0.0130,\n",
            "        0.0218, 0.0027, 0.0318, 0.0446, 0.0136, 0.0044, 0.0108, 0.0190, 0.1141],\n",
            "       grad_fn=<SelectBackward0>)\n",
            "label (actual next character): 1\n",
            "probability assigned by the net to the the correct character: 0.08815080672502518\n",
            "log likelihood: -2.428706169128418\n",
            "negative log likelihood: 2.428706169128418\n",
            "--------\n",
            "bigram example 2: mar (indexes 367,18)\n",
            "input to the neural net: 367\n",
            "output probabilities from the neural net: tensor([0.0019, 0.0160, 0.0289, 0.0162, 0.0292, 0.0312, 0.0180, 0.1052, 0.0098,\n",
            "        0.0203, 0.2766, 0.0084, 0.0527, 0.0713, 0.0164, 0.0188, 0.0367, 0.0048,\n",
            "        0.0438, 0.0341, 0.0231, 0.0177, 0.0232, 0.0178, 0.0081, 0.0275, 0.0422],\n",
            "       grad_fn=<SelectBackward0>)\n",
            "label (actual next character): 18\n",
            "probability assigned by the net to the the correct character: 0.04381975904107094\n",
            "log likelihood: -3.1276705265045166\n",
            "negative log likelihood: 3.1276705265045166\n",
            "--------\n",
            "bigram example 3: ary (indexes 48,25)\n",
            "input to the neural net: 48\n",
            "output probabilities from the neural net: tensor([0.0429, 0.0040, 0.0341, 0.0249, 0.0266, 0.0129, 0.0049, 0.0208, 0.0607,\n",
            "        0.0327, 0.0446, 0.0059, 0.0141, 0.0535, 0.0442, 0.0326, 0.1325, 0.0316,\n",
            "        0.0417, 0.1832, 0.0313, 0.0073, 0.0673, 0.0204, 0.0046, 0.0087, 0.0118],\n",
            "       grad_fn=<SelectBackward0>)\n",
            "label (actual next character): 25\n",
            "probability assigned by the net to the the correct character: 0.008742544800043106\n",
            "log likelihood: -4.739553928375244\n",
            "negative log likelihood: 4.739553928375244\n",
            "--------\n",
            "bigram example 4: ry. (indexes 531,0)\n",
            "input to the neural net: 531\n",
            "output probabilities from the neural net: tensor([0.0111, 0.0425, 0.0157, 0.0994, 0.0162, 0.0123, 0.0497, 0.0177, 0.0034,\n",
            "        0.0048, 0.0114, 0.0182, 0.0036, 0.0643, 0.0222, 0.0203, 0.2644, 0.0440,\n",
            "        0.0030, 0.0098, 0.0046, 0.0057, 0.0233, 0.1424, 0.0371, 0.0267, 0.0262],\n",
            "       grad_fn=<SelectBackward0>)\n",
            "label (actual next character): 0\n",
            "probability assigned by the net to the the correct character: 0.011137566529214382\n",
            "log likelihood: -4.497431755065918\n",
            "negative log likelihood: 4.497431755065918\n",
            "=========\n",
            "average negative log likelihood, i.e. loss = 3.698340654373169\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize\n",
        "# randomly initialize 27 neurons' weights. each neuron receives 27 inputs\n",
        "g = torch.Generator().manual_seed(2147483647)\n",
        "W = torch.randn((729, 27), generator=g, requires_grad=True)\n",
        "\n",
        "# create the training set of trigrams (x,y)\n",
        "xs, ys = [], []\n",
        "for w in words[:1]:\n",
        "  chs = ['.'] + list(w) + ['.']\n",
        "  for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
        "    ix1 = btoi[ch1+ch2]\n",
        "    ix3 = stoi[ch3]\n",
        "    xs.append(ix1)\n",
        "    ys.append(ix3)\n",
        "xs = torch.tensor(xs)\n",
        "ys = torch.tensor(ys)\n",
        "\n",
        "for k in range(100):\n",
        "  # forward pass\n",
        "  xenc = F.one_hot(xs, num_classes=729).float() # input to the network: one-hot encoding\n",
        "  logits = xenc @ W # predict log-counts\n",
        "  counts = logits.exp() # counts, equivalent to N\n",
        "  probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
        "  loss = -probs[torch.arange(4), ys].log().mean()\n",
        "  print(f\"Loss: \",loss.item())\n",
        "\n",
        "  # backward pass\n",
        "  # Calculated by pytoch by computing gradients\n",
        "  W.grad = None # set to zero the gradient\n",
        "  loss.backward()\n",
        "  W.data += -10 * W.grad"
      ],
      "metadata": {
        "id": "AOF8XEaVjo8E",
        "outputId": "aba5219a-277d-4593-f827-92f77c4a8a9c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss:  3.698340654373169\n",
            "Loss:  1.4152169227600098\n",
            "Loss:  0.3756910562515259\n",
            "Loss:  0.1751929223537445\n",
            "Loss:  0.11682480573654175\n",
            "Loss:  0.0881984755396843\n",
            "Loss:  0.07101797312498093\n",
            "Loss:  0.05951263755559921\n",
            "Loss:  0.05125107988715172\n",
            "Loss:  0.045023269951343536\n",
            "Loss:  0.04015641286969185\n",
            "Loss:  0.03624660521745682\n",
            "Loss:  0.0330355241894722\n",
            "Loss:  0.030350370332598686\n",
            "Loss:  0.028071332722902298\n",
            "Loss:  0.026112491264939308\n",
            "Loss:  0.024410486221313477\n",
            "Loss:  0.022917872294783592\n",
            "Loss:  0.02159806340932846\n",
            "Loss:  0.020422542467713356\n",
            "Loss:  0.019369103014469147\n",
            "Loss:  0.01841919682919979\n",
            "Loss:  0.017558693885803223\n",
            "Loss:  0.01677507534623146\n",
            "Loss:  0.016058772802352905\n",
            "Loss:  0.015401318669319153\n",
            "Loss:  0.014795714989304543\n",
            "Loss:  0.014236079528927803\n",
            "Loss:  0.013717394322156906\n",
            "Loss:  0.013235338032245636\n",
            "Loss:  0.012786008417606354\n",
            "Loss:  0.012366300448775291\n",
            "Loss:  0.011973431333899498\n",
            "Loss:  0.011604836210608482\n",
            "Loss:  0.01125815138220787\n",
            "Loss:  0.010931715369224548\n",
            "Loss:  0.010623668320477009\n",
            "Loss:  0.010332714766263962\n",
            "Loss:  0.010057242587208748\n",
            "Loss:  0.009796082973480225\n",
            "Loss:  0.009548064321279526\n",
            "Loss:  0.00931248627603054\n",
            "Loss:  0.009088209830224514\n",
            "Loss:  0.008874524384737015\n",
            "Loss:  0.008670742623507977\n",
            "Loss:  0.008476005867123604\n",
            "Loss:  0.008289886638522148\n",
            "Loss:  0.008111823350191116\n",
            "Loss:  0.00794118084013462\n",
            "Loss:  0.00777773279696703\n",
            "Loss:  0.0076207360252738\n",
            "Loss:  0.007470025680959225\n",
            "Loss:  0.007325146347284317\n",
            "Loss:  0.007185826543718576\n",
            "Loss:  0.007051732856780291\n",
            "Loss:  0.00692251930013299\n",
            "Loss:  0.006797958165407181\n",
            "Loss:  0.006677927915006876\n",
            "Loss:  0.006561826448887587\n",
            "Loss:  0.006449863780289888\n",
            "Loss:  0.006341646425426006\n",
            "Loss:  0.00623705517500639\n",
            "Loss:  0.006135878153145313\n",
            "Loss:  0.006037799175828695\n",
            "Loss:  0.005942893214523792\n",
            "Loss:  0.005850964225828648\n",
            "Loss:  0.005761755630373955\n",
            "Loss:  0.005675237625837326\n",
            "Loss:  0.005591364577412605\n",
            "Loss:  0.005509896203875542\n",
            "Loss:  0.005430711433291435\n",
            "Loss:  0.005353855434805155\n",
            "Loss:  0.00527914660051465\n",
            "Loss:  0.005206496454775333\n",
            "Loss:  0.005135797895491123\n",
            "Loss:  0.0050670369528234005\n",
            "Loss:  0.005000002682209015\n",
            "Loss:  0.004934799857437611\n",
            "Loss:  0.004871233832091093\n",
            "Loss:  0.0048093488439917564\n",
            "Loss:  0.00474893394857645\n",
            "Loss:  0.004690080881118774\n",
            "Loss:  0.0046326229348778725\n",
            "Loss:  0.004576604813337326\n",
            "Loss:  0.004521982744336128\n",
            "Loss:  0.004468576051294804\n",
            "Loss:  0.004416384268552065\n",
            "Loss:  0.004365437664091587\n",
            "Loss:  0.004315630532801151\n",
            "Loss:  0.004267023876309395\n",
            "Loss:  0.004219436086714268\n",
            "Loss:  0.004172809422016144\n",
            "Loss:  0.004127291031181812\n",
            "Loss:  0.004082807805389166\n",
            "Loss:  0.0040392689406871796\n",
            "Loss:  0.003996734507381916\n",
            "Loss:  0.003954860381782055\n",
            "Loss:  0.003914033994078636\n",
            "Loss:  0.00387392845004797\n",
            "Loss:  0.0038347067311406136\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DbeRDg5FoN59"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "E02: split up the dataset randomly into 80% train set, 10% dev set, 10% test set. Train the bigram and trigram models only on the training set. Evaluate them on dev and test splits. What can you see?"
      ],
      "metadata": {
        "id": "1E_da2qIEtE7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 2"
      ],
      "metadata": {
        "id": "r8n7rr1gc9PZ"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "E03: use the dev set to tune the strength of smoothing (or regularization) for the trigram model - i.e. try many possibilities and see which one works best based on the dev set loss. What patterns can you see in the train and dev set loss as you tune this strength? Take the best setting of the smoothing and evaluate on the test set once and at the end. How good of a loss do you achieve?"
      ],
      "metadata": {
        "id": "Nm5lYMRQEwyJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 1"
      ],
      "metadata": {
        "id": "FuLMP9Atc_g7"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "E04: we saw that our 1-hot vectors merely select a row of W, so producing these vectors explicitly feels wasteful. Can you delete our use of F.one_hot in favor of simply indexing into rows of W?"
      ],
      "metadata": {
        "id": "OOzpQ4y7Ez99"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "E05: look up and use F.cross_entropy instead. You should achieve the same result. Can you think of why we'd prefer to use F.cross_entropy instead?"
      ],
      "metadata": {
        "id": "8VFHbjP8E1Zn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "E06: meta-exercise! Think of a fun/interesting exercise and complete it."
      ],
      "metadata": {
        "id": "E3yrXzL7E2oR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bd75241zCTcw"
      },
      "outputs": [],
      "source": []
    }
  ]
}